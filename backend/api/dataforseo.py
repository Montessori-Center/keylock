# api/dataforseo.py - –ü–û–õ–ù–û–°–¢–¨–Æ –ü–ï–†–ï–†–ê–ë–û–¢–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
import sys
import json
import time
import uuid
from typing import Dict
from flask import Blueprint, request, jsonify, Response, stream_with_context
from config import Config
import pymysql
from datetime import datetime
from services.dataforseo_client import get_dataforseo_client, DataForSeoClient
from api.keywords import get_random_batch_color

dataforseo_bp = Blueprint('dataforseo', __name__)

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∑–∞–¥–∞—á
SERP_PROGRESS = {}

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π flush –¥–ª—è –ª–æ–≥–æ–≤
def log_print(*args, **kwargs):
    print(*args, **kwargs)
    sys.stdout.flush()

def get_db_connection():
    """–°–æ–∑–¥–∞—ë—Ç –ø—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î"""
    return pymysql.connect(
        host=Config.DB_HOST,
        port=Config.DB_PORT,
        user=Config.DB_USER,
        password=Config.DB_PASSWORD,
        database=Config.DB_NAME,
        cursorclass=pymysql.cursors.DictCursor
    )

def update_progress(task_id: str, current: int, total: int, keyword: str = '', status: str = 'processing'):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–¥–∞—á–∏"""
    SERP_PROGRESS[task_id] = {
        'current': current,
        'total': total,
        'keyword': keyword,
        'status': status,
        'timestamp': time.time()
    }
    log_print(f"üìä Progress updated: {current}/{total} - {keyword}")

def get_progress(task_id: str) -> Dict:
    """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–¥–∞—á–∏"""
    return SERP_PROGRESS.get(task_id, {
        'current': 0,
        'total': 0,
        'keyword': '',
        'status': 'not_found'
    })

def cleanup_progress(task_id: str):
    """–û—á–∏—â–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–¥–∞—á–∏"""
    if task_id in SERP_PROGRESS:
        del SERP_PROGRESS[task_id]
        log_print(f"üßπ Progress cleaned up for task {task_id}")
        
@dataforseo_bp.route('/apply-serp-sse', methods=['GET'])
def apply_serp_sse():
    """SSE endpoint –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ SERP –∞–Ω–∞–ª–∏–∑–∞"""
    task_id = request.args.get('task_id')
    
    if not task_id:
        return jsonify({'error': 'task_id required'}), 400
    
    def generate():
        """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–±—ã—Ç–∏–π SSE"""
        log_print(f"üîÑ SSE stream started for task {task_id}")
        
        # –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ (–º–∞–∫—Å 5 —Å–µ–∫)
        wait_time = 0
        while task_id not in SERP_PROGRESS and wait_time < 5:
            time.sleep(0.1)
            wait_time += 0.1
        
        if task_id not in SERP_PROGRESS:
            yield f"data: {json.dumps({'type': 'error', 'message': 'Task not found'})}\n\n"
            return
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        last_current = -1
        max_wait = 300  # 5 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
        start_time = time.time()
        
        while True:
            if time.time() - start_time > max_wait:
                yield f"data: {json.dumps({'type': 'error', 'message': 'Timeout'})}\n\n"
                break
            
            progress = get_progress(task_id)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–∑–º–µ–Ω–∏–ª—Å—è
            if progress['current'] != last_current:
                last_current = progress['current']
                
                if progress['status'] == 'processing':
                    event_data = {
                        'type': 'progress',
                        'current': progress['current'],
                        'total': progress['total'],
                        'keyword': progress['keyword']
                    }
                    yield f"data: {json.dumps(event_data)}\n\n"
                    
                elif progress['status'] == 'complete':
                    event_data = {
                        'type': 'complete',
                        'message': progress.get('message', 'Completed'),
                        'result': progress.get('result', {})
                    }
                    yield f"data: {json.dumps(event_data)}\n\n"
                    cleanup_progress(task_id)
                    break
                    
                elif progress['status'] == 'error':
                    event_data = {
                        'type': 'error',
                        'message': progress.get('error', 'Unknown error')
                    }
                    yield f"data: {json.dumps(event_data)}\n\n"
                    cleanup_progress(task_id)
                    break
            
            time.sleep(0.5)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 0.5 —Å–µ–∫—É–Ω–¥—ã
        
        log_print(f"üîö SSE stream ended for task {task_id}")
    
    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no'
        }
    )

@dataforseo_bp.route('/test', methods=['GET', 'POST'])
def test_endpoint():
    """–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç endpoint"""
    try:
        log_print("üß™ TEST ENDPOINT –í–´–ó–í–ê–ù!")
        log_print(f"   Method: {request.method}")
        if request.method == 'POST':
            data = request.get_json()
            log_print(f"   Data: {data}")
        
        return jsonify({
            'success': True,
            'message': 'Test endpoint —Ä–∞–±–æ—Ç–∞–µ—Ç!',
            'method': request.method,
            'timestamp': str(datetime.utcnow())
        })
    except Exception as e:
        log_print(f"‚ùå Test endpoint error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@dataforseo_bp.route('/get-keywords-simple', methods=['POST'])
def get_keywords_simple():
    """–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –±–µ–∑ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ DataForSeo"""
    try:
        log_print("üß™ SIMPLE TEST –í–´–ó–í–ê–ù!")
        data = request.get_json()
        log_print(f"   –ü–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {list(data.keys()) if data else 'None'}")
        
        # –ò–º–∏—Ç–∏—Ä—É–µ–º —Ä–∞–±–æ—Ç—É
        import time
        time.sleep(1)
        
        return jsonify({
            'success': True,
            'message': '–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ!',
            'received_keys': list(data.keys()) if data else [],
            'stats': {
                'total_results': 5,
                'added': 3,
                'updated': 2,
                'errors': 0,
                'cost': 0.00
            }
        })
    except Exception as e:
        log_print(f"‚ùå Simple test error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@dataforseo_bp.route('/get-keywords', methods=['POST'])
def get_new_keywords():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –≤—ã–¥–∞—á–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ DataForSeo"""
    connection = None
    
    log_print("=" * 50)
    log_print("üöÄ GET-KEYWORDS ENDPOINT –í–´–ó–í–ê–ù!")
    log_print("=" * 50)
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å–∞
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data in request'}), 400
        
        seed_keywords = data.get('seed_keywords', [])
        ad_group_id = data.get('ad_group_id')
        
        log_print(f"üîë Seed keywords count: {len(seed_keywords)}")
        log_print(f"üè∑Ô∏è  Ad group ID: {ad_group_id}")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã API —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        location_code = data.get('location_code', 2804)
        language_code = data.get('language_code', 'ru')
        limit = data.get('limit', 700)
        search_partners = data.get('search_partners', False)
        include_seed_keyword = data.get('include_seed_keyword', True)
        sort_by = data.get('sort_by', 'relevance')
        date_from = data.get('date_from', '2024-01-01')
        date_to = data.get('date_to')
        
        if not seed_keywords:
            return jsonify({'success': False, 'error': 'No seed keywords provided'}), 400
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
        connection = get_db_connection()
        cursor = connection.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –≥—Ä—É–ø–ø—É –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        cursor.execute("SELECT * FROM ad_groups WHERE id = %s", (ad_group_id,))
        ad_group = cursor.fetchone()
        
        if not ad_group:
            return jsonify({'success': False, 'error': 'Ad group not found'}), 404
        
        campaign_id = ad_group['campaign_id']
        
        # –ó–∞–ø—Ä–æ—Å –∫ DataForSeo
        try:
            dataforseo_client = get_dataforseo_client()
            
            response = dataforseo_client.get_keywords_for_keywords(
                keywords=seed_keywords,
                location_code=location_code,
                language_code=language_code,
                search_partners=search_partners,
                sort_by=sort_by,
                limit=limit,
                include_seed_keyword=include_seed_keyword,
                date_from=date_from,
                date_to=date_to
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–≤–µ—Ç
            if not response.get('tasks'):
                return jsonify({
                    'success': False,
                    'error': 'No data received from DataForSeo'
                }), 500
            
            task = response['tasks'][0]
            if task.get('status_code') != 20000:
                error_msg = task.get('status_message', 'Unknown error')
                return jsonify({
                    'success': False,
                    'error': f"DataForSeo error: {error_msg}"
                }), 500
            
            request_cost = task.get('cost', 0.05)
            log_print(f"üí∞ –°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞: ${request_cost}")
            
        except ValueError as e:
            return jsonify({
                'success': False,
                'error': f'DataForSeo API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {str(e)}'
            }), 400
        except Exception as e:
            return jsonify({
                'success': False,
                'error': f'DataForSeo API error: {str(e)}'
            }), 500
        
        # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
        try:
            keywords_data = dataforseo_client.parse_keywords_response(response)
            log_print(f"üìà –ü–æ–ª—É—á–µ–Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(keywords_data)}")
        except Exception as e:
            log_print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")
            keywords_data = []
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ü–≤–µ—Ç –¥–ª—è –Ω–æ–≤–æ–π –ø–∞—Ä—Ç–∏–∏
        batch_color = get_random_batch_color()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –ë–î
        added_count = 0
        updated_count = 0
        errors = []
        
        for kw_data in keywords_data:
            try:
                keyword_text = kw_data['keyword']
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
                cursor.execute(
                    "SELECT * FROM keywords WHERE ad_group_id = %s AND keyword = %s",
                    (ad_group_id, keyword_text)
                )
                existing = cursor.fetchone()
                
                if existing:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ (—Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
                    update_query = """
                        UPDATE keywords SET 
                            avg_monthly_searches = %s,
                            competition = %s,
                            competition_percent = %s,
                            min_top_of_page_bid = %s,
                            max_top_of_page_bid = %s,
                            three_month_change = %s,
                            yearly_change = %s,
                            max_cpc = %s,
                            updated_at = NOW()
                        WHERE id = %s
                    """
                    
                    update_data = (
                        kw_data.get('avg_monthly_searches', 0),
                        kw_data.get('competition', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                        kw_data.get('competition_percent', 0),
                        kw_data.get('min_top_of_page_bid', 0),
                        kw_data.get('max_top_of_page_bid', 0),
                        kw_data.get('three_month_change'),
                        kw_data.get('yearly_change'),
                        kw_data.get('cpc', existing['max_cpc']),
                        existing['id']
                    )
                    
                    cursor.execute(update_query, update_data)
                    updated_count += 1
                else:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ
                    insert_query = """
                        INSERT INTO keywords (
                            campaign_id, ad_group_id, keyword, criterion_type, status,
                            avg_monthly_searches, competition, competition_percent,
                            min_top_of_page_bid, max_top_of_page_bid, three_month_change,
                            yearly_change, max_cpc, intent_type, is_new, batch_color,
                            has_ads, has_school_sites, has_google_maps, has_our_site
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """
                    
                    insert_data = (
                        campaign_id,
                        ad_group_id,
                        keyword_text,
                        'Phrase',
                        'Enabled',
                        kw_data.get('avg_monthly_searches', 0),
                        kw_data.get('competition', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                        kw_data.get('competition_percent', 0),
                        kw_data.get('min_top_of_page_bid', 0),
                        kw_data.get('max_top_of_page_bid', 0),
                        kw_data.get('three_month_change'),
                        kw_data.get('yearly_change'),
                        kw_data.get('cpc', 3.61),
                        '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π',  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ —á–µ—Ä–µ–∑ SERP
                        True,  # is_new
                        batch_color,  # batch_color
                        False,  # has_ads - –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        False,  # has_school_sites - –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        False,  # has_google_maps - –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        False   # has_our_site - –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    )
                    
                    cursor.execute(insert_query, insert_data)
                    added_count += 1
                    
            except Exception as e:
                errors.append(f"Error processing '{kw_data.get('keyword', 'unknown')}': {str(e)}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        connection.commit()
        
        result = {
            'success': True,
            'message': f'–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(keywords_data)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤. –î–æ–±–∞–≤–ª–µ–Ω–æ: {added_count}, –æ–±–Ω–æ–≤–ª–µ–Ω–æ: {updated_count}',
            'stats': {
                'total_results': len(keywords_data),
                'added': added_count,
                'updated': updated_count,
                'errors': len(errors),
                'cost': request_cost
            },
            'cost': request_cost
        }
        
        if errors:
            result['errors'] = errors[:10]
        
        return jsonify(result)
        
    except Exception as e:
        if connection:
            connection.rollback()
        log_print(f"üí• –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if connection:
            connection.close()

@dataforseo_bp.route('/apply-serp', methods=['POST'])
def apply_serp_analysis():
    """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SERP –∞–Ω–∞–ª–∏–∑–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —á–µ—Ä–µ–∑ SSE"""
    
    try:
        data = request.json
        keyword_ids = data.get('keyword_ids', [])
        task_id = data.get('task_id') or str(uuid.uuid4())
        
        log_print(f"\n{'='*50}")
        log_print(f"üöÄ SERP Analysis started: task_id={task_id}")
        log_print(f"   Keywords: {len(keyword_ids)}")
        log_print(f"{'='*50}")
        
        if not keyword_ids:
            return jsonify({'success': False, 'error': 'No keywords selected'}), 400
        
        # –ò–ó–ú–ï–ù–ï–ù–û: –î–ª—è 1 —Å–ª–æ–≤–∞ - —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –±–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞, –¥–ª—è 2+ - Task-–≤–µ—Ä—Å–∏—è
        if len(keyword_ids) == 1:
            # Live-–∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ (–±—ã—Å—Ç—Ä–æ, –±–µ–∑ –º–æ–¥–∞–ª–∫–∏)
            try:
                result = process_serp_sync(task_id, keyword_ids, data)
                return jsonify(result), 200
            except Exception as e:
                log_print(f"‚ùå Live analysis error: {str(e)}")
                return jsonify({
                    'success': False,
                    'error': f'SERP analysis failed: {str(e)}'
                }), 500
        else:
            # Task-–≤–µ—Ä—Å–∏—è –¥–ª—è 2+ —Å–ª–æ–≤ (—Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º —á–µ—Ä–µ–∑ SSE)
            import threading
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            update_progress(task_id, 0, len(keyword_ids), '', 'processing')
            
            thread = threading.Thread(
                target=process_serp_async,
                args=(task_id, keyword_ids, data)
            )
            thread.daemon = True
            thread.start()
            
            return jsonify({
                'success': True,
                'task_id': task_id,
                'message': 'SERP analysis started',
                'use_sse': True
            }), 200
            
    except Exception as e:
        log_print(f"‚ùå Error in apply_serp_analysis: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return jsonify({
            'success': False, 
            'error': str(e)
        }), 500
            
def process_serp_sync(task_id: str, keyword_ids: list, params: dict) -> dict:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ SERP —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ our_organic_position –∏ our_actual_position
    """
    connection = None
    cursor = None
    
    try:
        connection = get_db_connection()
        cursor = connection.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        placeholders = ','.join(['%s'] * len(keyword_ids))
        cursor.execute(f"""
            SELECT k.id, k.keyword, k.campaign_id 
            FROM keywords k
            WHERE k.id IN ({placeholders})
        """, keyword_ids)
        keywords_data = cursor.fetchall()
        
        if not keywords_data:
            return {'success': False, 'error': 'Keywords not found'}
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∏–µ–Ω—Ç DataForSeo
        try:
            dataforseo_client = get_dataforseo_client()
        except ValueError as e:
            return {'success': False, 'error': f'DataForSeo API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {str(e)}'}
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã SERP –∑–∞–ø—Ä–æ—Å–∞
        serp_params = {
            'location_code': params.get('location_code', 2804),
            'language_code': params.get('language_code', 'ru'),
            'device': params.get('device', 'desktop'),
            'os': params.get('os', 'windows'),
            'depth': params.get('depth', 100),
            'calculate_rectangles': params.get('calculate_rectangles', False),
            'browser_screen_width': params.get('browser_screen_width', 1920),
            'browser_screen_height': params.get('browser_screen_height', 1080),
            'se_domain': params.get('se_domain', 'google.com.ua')
        }
        
        updated_count = 0
        errors = []
        total_cost = 0.0
        results_summary = {
            'with_ads': 0,
            'with_maps': 0,
            'with_our_site': 0,
            'with_school_sites': 0,
            'commercial_intent': 0
        }
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        for idx, kw in enumerate(keywords_data):
            try:
                log_print(f"\nüîç –ê–Ω–∞–ª–∏–∑ [{idx+1}/{len(keywords_data)}]: {kw['keyword']}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ü–ï–†–ï–î –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
                update_progress(task_id, idx, len(keywords_data), kw['keyword'], 'processing')
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º SERP –∑–∞–ø—Ä–æ—Å
                serp_response = dataforseo_client.get_serp(
                    keyword=kw['keyword'],
                    **serp_params
                )
                
                # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é parse_serp_response)
                serp_data = parse_serp_response(
                    serp_response, 
                    kw['campaign_id'], 
                    connection,
                    keyword_id=kw['id'],
                    keyword_text=kw['keyword']
                )
                
                if serp_data:
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –° –ü–û–ó–ò–¶–ò–Ø–ú–ò
                    cursor.execute("""
                        UPDATE keywords 
                        SET 
                            has_ads = %s,
                            has_school_sites = %s,
                            has_google_maps = %s,
                            has_our_site = %s,
                            intent_type = %s,
                            our_organic_position = %s,
                            our_actual_position = %s,
                            last_serp_check = NOW(),
                            updated_at = NOW()
                        WHERE id = %s
                    """, (
                        serp_data['has_ads'],
                        serp_data['has_school_sites'],
                        serp_data['has_google_maps'],
                        serp_data['has_our_site'],
                        serp_data['intent_type'],
                        serp_data.get('our_organic_position'),  # –ù–û–í–û–ï
                        serp_data.get('our_actual_position'),   # –ù–û–í–û–ï
                        kw['id']
                    ))
                    
                    updated_count += 1
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    log_print(f"   ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤ –ë–î")
                    if serp_data.get('has_our_site'):
                        log_print(f"   üìç –ü–æ–∑–∏—Ü–∏–∏: –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∞—è={serp_data.get('our_organic_position')}, —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∞—è={serp_data.get('our_actual_position')}")
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    if serp_data['has_ads']:
                        results_summary['with_ads'] += 1
                    if serp_data['has_google_maps']:
                        results_summary['with_maps'] += 1
                    if serp_data['has_our_site']:
                        results_summary['with_our_site'] += 1
                    if serp_data['has_school_sites']:
                        results_summary['with_school_sites'] += 1
                    if serp_data['intent_type'] == '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π':
                        results_summary['commercial_intent'] += 1
                    
                    # –°—á–∏—Ç–∞–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å
                    if serp_response.get('tasks'):
                        task_cost = serp_response['tasks'][0].get('cost', 0)
                        total_cost += task_cost
                        log_print(f"   üí∞ –°—Ç–æ–∏–º–æ—Å—Ç—å: ${task_cost:.4f}")
                else:
                    error_msg = f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è '{kw['keyword']}'"
                    errors.append(error_msg)
                    log_print(f"   ‚ö†Ô∏è {error_msg}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ü–û–°–õ–ï –æ–±—Ä–∞–±–æ—Ç–∫–∏
                update_progress(task_id, idx + 1, len(keywords_data), kw['keyword'], 'processing')
                
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –¥–ª—è '{kw['keyword']}': {str(e)}"
                log_print(f"   ‚ùå {error_msg}")
                errors.append(error_msg)
                import traceback
                traceback.print_exc()
                
                # –í—Å—ë —Ä–∞–≤–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø—Ä–∏ –æ—à–∏–±–∫–µ
                update_progress(task_id, idx + 1, len(keywords_data), kw['keyword'], 'processing')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        connection.commit()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        message = f'SERP –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {updated_count} –∏–∑ {len(keywords_data)} —Å–ª–æ–≤'
        
        log_print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        log_print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {updated_count}/{len(keywords_data)}")
        log_print(f"   –û—à–∏–±–æ–∫: {len(errors)}")
        log_print(f"   –°—Ç–æ–∏–º–æ—Å—Ç—å: ${total_cost:.4f}")
        log_print(f"   –° —Ä–µ–∫–ª–∞–º–æ–π: {results_summary['with_ads']}")
        log_print(f"   –° –∫–∞—Ä—Ç–∞–º–∏: {results_summary['with_maps']}")
        log_print(f"   –° –Ω–∞—à–∏–º —Å–∞–π—Ç–æ–º: {results_summary['with_our_site']}")
        log_print(f"   –° —Å–∞–π—Ç–∞–º–∏ —à–∫–æ–ª: {results_summary['with_school_sites']}")
        log_print(f"   –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ–Ω—Ç: {results_summary['commercial_intent']}")
        
        return {
            'success': True,
            'message': message,
            'updated': updated_count,
            'total': len(keywords_data),
            'errors': errors[:10] if errors else [],
            'cost': round(total_cost, 4),
            'summary': results_summary
        }
        
    except Exception as e:
        if connection:
            try:
                connection.rollback()
            except:
                pass
        log_print(f"‚ùå Error in process_serp_sync: {str(e)}")
        import traceback
        traceback.print_exc()
        raise
        
    finally:
        if cursor:
            try:
                cursor.close()
            except:
                pass
        if connection:
            try:
                connection.close()
            except:
                pass
            
def process_serp_async(task_id: str, keyword_ids: list, params: dict):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ SERP –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ (2+ —Å–ª–æ–≤)"""
    try:
        log_print(f"üîÑ Async SERP processing started for task {task_id}")
        
        # –í—ã–∑—ã–≤–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        result = process_serp_sync(task_id, keyword_ids, params)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        SERP_PROGRESS[task_id].update({
            'status': 'complete',
            'result': result,
            'message': result.get('message', 'Completed')
        })
        
        log_print(f"‚úÖ Async SERP processing completed for task {task_id}")
        
    except Exception as e:
        log_print(f"‚ùå Async SERP error for task {task_id}: {str(e)}")
        import traceback
        traceback.print_exc()
        
        SERP_PROGRESS[task_id].update({
            'status': 'error',
            'error': str(e),
            'message': f'Error: {str(e)}'
        })
        

def _process_serp_batch(keywords_data, serp_params, connection, dataforseo_client):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ SERP —á–µ—Ä–µ–∑ task_post –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤"""
    
    log_print(f"üöÄ Batch SERP –¥–ª—è {len(keywords_data)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
    
    # –®–∞–≥ 1: –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ task_post
    task_ids = []
    batch_size = 100  # –ú–∞–∫—Å–∏–º—É–º –∑–∞–¥–∞—á –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ
    
    for i in range(0, len(keywords_data), batch_size):
        batch = keywords_data[i:i+batch_size]
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è batch –∑–∞–ø—Ä–æ—Å–∞
        tasks = []
        for kw in batch:
            task_data = {
                "keyword": kw['keyword'],
                "tag": f"keyword_id_{kw['id']}",  # –¢–µ–≥ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
                **serp_params
            }
            tasks.append(task_data)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º batch –∑–∞–ø—Ä–æ—Å
        response = dataforseo_client.post_serp_tasks(tasks)
        
        if response.get('tasks'):
            for task in response['tasks']:
                if task.get('id'):
                    task_ids.append({
                        'task_id': task['id'],
                        'keyword_id': int(task.get('data', {}).get('tag', '').replace('keyword_id_', '')),
                        'keyword': next((kw['keyword'] for kw in batch if str(kw['id']) in task.get('data', {}).get('tag', '')), '')
                    })
        
        log_print(f"üìã –°–æ–∑–¥–∞–Ω–æ {len(task_ids)} –∑–∞–¥–∞—á")
    
    # –®–∞–≥ 2: –ñ–¥–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á
    import time
    max_wait = 120  # –ú–∞–∫—Å–∏–º—É–º 2 –º–∏–Ω—É—Ç—ã –æ–∂–∏–¥–∞–Ω–∏—è
    start_time = time.time()
    completed_tasks = []
    
    while len(completed_tasks) < len(task_ids) and time.time() - start_time < max_wait:
        time.sleep(2)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 2 —Å–µ–∫—É–Ω–¥—ã
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á
        ready_tasks = dataforseo_client.get_tasks_ready()
        
        for task_info in task_ids:
            if task_info['task_id'] not in [t['task_id'] for t in completed_tasks]:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏
                if task_info['task_id'] in ready_tasks:
                    completed_tasks.append(task_info)
                    log_print(f"‚úÖ –ó–∞–¥–∞—á–∞ –≥–æ—Ç–æ–≤–∞: {task_info['keyword'][:30]}...")
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –Ω–∞ frontend —á–µ—Ä–µ–∑ SSE –∏–ª–∏ WebSocket
        progress_percent = (len(completed_tasks) / len(task_ids)) * 100
        log_print(f"‚è≥ –ü—Ä–æ–≥—Ä–µ—Å—Å: {len(completed_tasks)}/{len(task_ids)} ({progress_percent:.0f}%)")
    
    # –®–∞–≥ 3: –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    updated_count = 0
    errors = []
    total_cost = 0
    
    for task_info in completed_tasks:
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–¥–∞—á–∏
            result = dataforseo_client.get_task_result(task_info['task_id'])
            
            # –ü–∞—Ä—Å–∏–º –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –ë–î
            serp_data = parse_serp_response(
                result,
                keywords_data[0]['campaign_id'],
                connection,
                keyword_id=task_info['keyword_id'],
                keyword_text=task_info['keyword']
            )
            
            if serp_data:
                cursor = connection.cursor()
                cursor.execute("""
                    UPDATE keywords 
                    SET 
                        has_ads = %s,
                        has_school_sites = %s,
                        has_google_maps = %s,
                        has_our_site = %s,
                        intent_type = %s,
                        last_serp_check = NOW(),
                        updated_at = NOW()
                    WHERE id = %s
                """, (
                    serp_data['has_ads'],
                    serp_data['has_school_sites'],
                    serp_data['has_google_maps'],
                    serp_data['has_our_site'],
                    serp_data['intent_type'],
                    task_info['keyword_id']
                ))
                cursor.close()
                updated_count += 1
                
                # –°—á–∏—Ç–∞–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å
                if result.get('cost'):
                    total_cost += result['cost']
                    
        except Exception as e:
            errors.append(f"–û—à–∏–±–∫–∞ –¥–ª—è '{task_info['keyword']}': {str(e)}")
    
    connection.commit()
    
    return jsonify({
        'success': True,
        'message': f'Batch SERP –∑–∞–≤–µ—Ä—à–µ–Ω! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {updated_count} –∏–∑ {len(keywords_data)} —Å–ª–æ–≤',
        'updated': updated_count,
        'total': len(keywords_data),
        'errors': errors[:10] if errors else [],
        'cost': round(total_cost, 4),
        'method': 'batch_task_post'
    })

def _process_serp_live(keywords_data, serp_params, connection, dataforseo_client):
    connection = None
    
    try:
        data = request.json
        keyword_ids = data.get('keyword_ids', [])
        
        if not keyword_ids:
            return jsonify({'success': False, 'error': 'No keywords selected'}), 400
        
        connection = get_db_connection()
        cursor = connection.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å campaign_id
        placeholders = ','.join(['%s'] * len(keyword_ids))
        cursor.execute(f"""
            SELECT k.id, k.keyword, k.campaign_id 
            FROM keywords k
            WHERE k.id IN ({placeholders})
        """, keyword_ids)
        keywords_data = cursor.fetchall()
        
        if not keywords_data:
            return jsonify({'success': False, 'error': 'Keywords not found'}), 404
        
        # –ü–æ–ª—É—á–∞–µ–º DataForSeo –∫–ª–∏–µ–Ω—Ç
        try:
            dataforseo_client = get_dataforseo_client()
        except ValueError as e:
            return jsonify({
                'success': False,
                'error': f'DataForSeo API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {str(e)}'
            }), 400
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã SERP –∑–∞–ø—Ä–æ—Å–∞
        serp_params = {
            'location_code': data.get('location_code', 2804),
            'language_code': data.get('language_code', 'ru'),
            'device': data.get('device', 'desktop'),
            'os': data.get('os', 'windows'),
            'depth': data.get('depth', 100),
            'calculate_rectangles': data.get('calculate_rectangles', False),
            'browser_screen_width': data.get('browser_screen_width', 1920),
            'browser_screen_height': data.get('browser_screen_height', 1080),
            'se_domain': data.get('se_domain', 'google.com.ua')
        }
        
        updated_count = 0
        errors = []
        total_cost = 0
        results_summary = {
            'with_ads': 0,
            'with_maps': 0,
            'with_our_site': 0,
            'with_school_sites': 0,
            'commercial_intent': 0
        }
        
        for idx, kw in enumerate(keywords_data):
            try:
                log_print(f"\nüîç –ê–Ω–∞–ª–∏–∑ [{idx+1}/{len(keywords_data)}]: {kw['keyword']}")
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º SERP –∑–∞–ø—Ä–æ—Å
                serp_response = dataforseo_client.get_serp(
                    keyword=kw['keyword'],
                    **serp_params
                )
                
                # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                serp_data = parse_serp_response(
                    serp_response, 
                    kw['campaign_id'], 
                    connection,
                    keyword_id=kw['id'],
                    keyword_text=kw['keyword']
                )
                
                if serp_data:
                    # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –Ω–∞—à–µ–≥–æ —Å–∞–π—Ç–∞
                    our_position = 0
                    if serp_response.get('tasks') and serp_response['tasks'][0].get('result'):
                        items = serp_response['tasks'][0]['result'][0].get('items', [])
                        for item in items:
                            if item.get('type') == 'organic':
                                domain = item.get('domain', '').lower().replace('www.', '')
                                # TODO: –ø–æ–ª—É—á–∞—Ç—å –¥–æ–º–µ–Ω –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑ campaign_sites
                                if domain == 'montessori.ua':
                                    our_position = item.get('rank_absolute', 0)
                                    break
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ë–î —Å –ø–æ–∑–∏—Ü–∏–µ–π –∏ –¥–∞—Ç–æ–π
                    cursor.execute("""
                        UPDATE keywords 
                        SET 
                            has_ads = %s,
                            has_school_sites = %s,
                            has_google_maps = %s,
                            has_our_site = %s,
                            intent_type = %s,
                            last_serp_check = NOW(),
                            serp_position = %s,
                            updated_at = NOW()
                        WHERE id = %s
                    """, (
                        serp_data['has_ads'],
                        serp_data['has_school_sites'],
                        serp_data['has_google_maps'],
                        serp_data['has_our_site'],
                        serp_data['intent_type'],
                        our_position if our_position > 0 else None,
                        kw['id']
                    ))
                    
                    updated_count += 1
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    if serp_data['has_ads']:
                        results_summary['with_ads'] += 1
                    if serp_data['has_google_maps']:
                        results_summary['with_maps'] += 1
                    if serp_data['has_our_site']:
                        results_summary['with_our_site'] += 1
                    if serp_data['has_school_sites']:
                        results_summary['with_school_sites'] += 1
                    if serp_data['intent_type'] == '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π':
                        results_summary['commercial_intent'] += 1
                    
                    # –°—á–∏—Ç–∞–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å
                    if serp_response.get('tasks'):
                        task_cost = serp_response['tasks'][0].get('cost', 0.003)
                        total_cost += task_cost
                    
                    log_print(f"   ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ")
                else:
                    error_msg = f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è '{kw['keyword']}'"
                    errors.append(error_msg)
                    log_print(f"   ‚ö†Ô∏è {error_msg}")
                    
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –¥–ª—è '{kw['keyword']}': {str(e)}"
                errors.append(error_msg)
                log_print(f"   ‚ùå {error_msg}")
                import traceback
                traceback.print_exc()
        
        connection.commit()
        cursor.close()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        message = f'SERP –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {updated_count} –∏–∑ {len(keywords_data)} —Å–ª–æ–≤'
        
        log_print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã SERP –∞–Ω–∞–ª–∏–∑–∞:")
        log_print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {updated_count}/{len(keywords_data)}")
        log_print(f"   –û—à–∏–±–æ–∫: {len(errors)}")
        log_print(f"   –°—Ç–æ–∏–º–æ—Å—Ç—å: ${total_cost:.4f}")
        
        return jsonify({
            'success': True,
            'message': message,
            'updated': updated_count,
            'total': len(keywords_data),
            'errors': errors[:10] if errors else [],
            'cost': round(total_cost, 4),
            'summary': results_summary
        })
        
    except Exception as e:
        if connection:
            connection.rollback()
        log_print(f"‚ùå Error in apply_serp_analysis: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500
        
    finally:
        if connection:
            connection.close()
            
@dataforseo_bp.route('/serp-logs', methods=['GET'])
def get_serp_logs():
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ SERP –ª–æ–≥–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
    
    Parameters:
    - limit: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50)
    - keyword_id: —Ñ–∏–ª—å—Ç—Ä –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É keyword_id
    - keyword_ids: —Å–ø–∏—Å–æ–∫ keyword_id —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–≤)
    - latest_only: –µ—Å–ª–∏ true, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ª–æ–≥ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ keyword_id
    """
    connection = None
    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        limit = request.args.get('limit', 50, type=int)
        keyword_id = request.args.get('keyword_id', None, type=int)
        keyword_ids_str = request.args.get('keyword_ids', None, type=str)
        latest_only = request.args.get('latest_only', 'false', type=str).lower() == 'true'
        
        log_print(f"üìä get_serp_logs called: limit={limit}, keyword_id={keyword_id}, keyword_ids={keyword_ids_str}, latest_only={latest_only}")
        
        connection = get_db_connection()
        cursor = connection.cursor()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º SQL –∑–∞–ø—Ä–æ—Å
        if latest_only and keyword_ids_str:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ª–æ–≥–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
            keyword_ids_list = [int(kid.strip()) for kid in keyword_ids_str.split(',') if kid.strip()]
            
            # –ü–æ–¥–∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è MAX(id) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ keyword_id
            placeholders = ','.join(['%s'] * len(keyword_ids_list))
            query = f"""
                SELECT sl.* FROM serp_logs sl
                INNER JOIN (
                    SELECT keyword_id, MAX(id) as max_id
                    FROM serp_logs
                    WHERE keyword_id IN ({placeholders})
                    GROUP BY keyword_id
                ) latest ON sl.id = latest.max_id
                ORDER BY sl.created_at DESC
            """
            cursor.execute(query, tuple(keyword_ids_list))
            
        elif keyword_ids_str:
            # –í—Å–µ –ª–æ–≥–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
            keyword_ids_list = [int(kid.strip()) for kid in keyword_ids_str.split(',') if kid.strip()]
            placeholders = ','.join(['%s'] * len(keyword_ids_list))
            query = f"""
                SELECT * FROM serp_logs 
                WHERE keyword_id IN ({placeholders})
                ORDER BY created_at DESC 
                LIMIT %s
            """
            cursor.execute(query, tuple(keyword_ids_list + [limit]))
            
        elif keyword_id:
            # –í—Å–µ –ª–æ–≥–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞
            cursor.execute("""
                SELECT * FROM serp_logs 
                WHERE keyword_id = %s 
                ORDER BY created_at DESC 
                LIMIT %s
            """, (keyword_id, limit))
            
        else:
            # –í—Å–µ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ª–æ–≥–∏
            cursor.execute("""
                SELECT * FROM serp_logs 
                ORDER BY created_at DESC 
                LIMIT %s
            """, (limit,))
        
        logs = cursor.fetchall()
        log_print(f"üìã Found {len(logs)} logs in DB")
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ª–æ–≥–∏ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        formatted_logs = []
        for log in logs:
            try:
                # –ü–∞—Ä—Å–∏–º JSON –ø–æ–ª—è
                analysis_result = {}
                organic_results = []
                paid_results = []
                
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ analysis_result (–Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç)
                if log.get('analysis_result'):
                    try:
                        analysis_result = json.loads(log['analysis_result'])
                    except:
                        log_print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å analysis_result –¥–ª—è log_id={log['id']}")
                
                # –ï—Å–ª–∏ analysis_result –ø—É—Å—Ç–æ–π, –ø—ã—Ç–∞–µ–º—Å—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–∑ —Å—Ç–∞—Ä—ã—Ö –ø–æ–ª–µ–π
                if not analysis_result:
                    analysis_result = {
                        'has_ads': log.get('has_ads', False),
                        'has_google_maps': log.get('has_maps', False),
                        'has_our_site': log.get('has_our_site', False),
                        'our_organic_position': None,  # –í —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å—è—Ö –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å
                        'our_actual_position': None,
                        'has_school_sites': log.get('has_school_sites', False),
                        'school_percentage': log.get('school_percentage', 0),
                        'intent_type': log.get('intent_type', '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π'),
                        'total_organic': log.get('organic_count', 0),
                        'paid_count': log.get('paid_count', 0),
                        'maps_count': log.get('maps_count', 0)
                    }
                
                # –ü–∞—Ä—Å–∏–º parsed_items
                if log.get('parsed_items'):
                    try:
                        parsed_items = json.loads(log['parsed_items'])
                        organic_results = parsed_items.get('organic', [])
                        paid_results = parsed_items.get('paid', [])
                    except:
                        log_print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å parsed_items –¥–ª—è log_id={log['id']}")
                
                formatted_log = {
                    'id': log['id'],
                    'keyword_id': log['keyword_id'],
                    'keyword_text': log['keyword_text'],
                    'created_at': log['created_at'].isoformat() if log.get('created_at') else None,
                    'cost': float(log.get('cost', 0)),
                    'analysis_result': {
                        'has_ads': analysis_result.get('has_ads', False),
                        'has_google_maps': analysis_result.get('has_google_maps', False),
                        'has_our_site': analysis_result.get('has_our_site', False),
                        'our_organic_position': analysis_result.get('our_organic_position'),
                        'our_actual_position': analysis_result.get('our_actual_position'),
                        'has_school_sites': analysis_result.get('has_school_sites', False),
                        'school_percentage': analysis_result.get('school_percentage', 0),
                        'intent_type': analysis_result.get('intent_type', '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π'),
                        'total_organic': analysis_result.get('total_organic', 0),
                        'paid_count': analysis_result.get('paid_count', 0),
                        'maps_count': analysis_result.get('maps_count', 0)
                    },
                    'organic_results': organic_results,
                    'paid_results': paid_results,
                    'raw_response': log.get('raw_response'),
                    'parsed_items': parsed_items,
                    'our_domain': None
                }
                
                formatted_logs.append(formatted_log)
                
            except Exception as e:
                log_print(f"‚ö†Ô∏è Error formatting log {log.get('id')}: {str(e)}")
                import traceback
                traceback.print_exc()
                continue
        
        cursor.close()
        
        log_print(f"üìä Returning {len(formatted_logs)} SERP logs")
        
        return jsonify({
            'success': True,
            'count': len(formatted_logs),
            'logs': formatted_logs,
            'filters_applied': {
                'keyword_id': keyword_id,
                'keyword_ids': keyword_ids_str,
                'latest_only': latest_only
            }
        })
        
    except Exception as e:
        log_print(f"‚ùå Error getting SERP logs: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if connection:
            connection.close()

@dataforseo_bp.route('/serp-logs/<int:log_id>', methods=['GET'])
def get_serp_log_details(log_id):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º SERP –∞–Ω–∞–ª–∏–∑–µ"""
    connection = None
    try:
        connection = get_db_connection()
        cursor = connection.cursor()
        
        cursor.execute("SELECT * FROM serp_logs WHERE id = %s", (log_id,))
        log = cursor.fetchone()
        
        if not log:
            return jsonify({'success': False, 'error': 'Log not found'}), 404
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä
        parsed_items = {}
        if log.get('parsed_items'):
            try:
                parsed_items = json.loads(log['parsed_items'])
            except:
                parsed_items = {}
        
        detailed_report = {
            'id': log['id'],
            'keyword': log.get('keyword_text', ''),
            'created_at': log['created_at'].isoformat() if log.get('created_at') else None,
            'parameters': {
                'location_code': log.get('location_code', 0),
                'language_code': log.get('language_code', ''),
                'device': log.get('device', ''),
                'depth': log.get('depth', 0)
            },
            'summary': {
                'total_items': log.get('total_items', 0),
                'organic_count': log.get('organic_count', 0),
                'paid_count': log.get('paid_count', 0),
                'maps_count': log.get('maps_count', 0)
            },
            'analysis_result': {
                'has_ads': bool(log.get('has_ads')),
                'has_maps': bool(log.get('has_maps')),
                'has_our_site': bool(log.get('has_our_site')),
                'has_school_sites': bool(log.get('has_school_sites')),
                'intent_type': log.get('intent_type', '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π'),
                'school_percentage': float(log.get('school_percentage', 0))
            },
            'cost': float(log.get('cost', 0)),
            'organic_results': parsed_items.get('organic', []),
            'paid_results': parsed_items.get('paid', []),
            'all_items': parsed_items.get('all_items', [])
        }
        
        cursor.close()
        
        return jsonify({
            'success': True,
            'report': detailed_report
        })
        
    except Exception as e:
        log_print(f"‚ùå Error getting SERP log details: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if connection:
            connection.close()

@dataforseo_bp.route('/test-connection', methods=['POST'])
def test_dataforseo_connection():
    """–¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ DataForSeo API"""
    try:
        data = request.json
        temp_client = DataForSeoClient(data['login'], data['password'])
        status = temp_client.get_status()
        
        if status.get('tasks') and status['tasks'][0].get('result'):
            balance = status['tasks'][0]['result'][0].get('money', {}).get('balance', 0)
            return jsonify({
                'success': True, 
                'message': f'API —Ä–∞–±–æ—Ç–∞–µ—Ç. –ë–∞–ª–∞–Ω—Å: ${balance}'
            })
        else:
            return jsonify({'success': False, 'message': '–ù–µ–≤–µ—Ä–Ω—ã–µ —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ'})
    except Exception as e:
        return jsonify({'success': False, 'message': f'–û—à–∏–±–∫–∞ API: {str(e)}'})
        
@dataforseo_bp.route('/check-serp-cost', methods=['POST'])
def check_serp_cost():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ SERP –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º"""
    try:
        data = request.json
        keyword_ids = data.get('keyword_ids', [])
        depth = data.get('depth', 100)
        
        # –†–∞—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏
        base_cost = 0.003  # $0.003 –∑–∞ SERP regular –¥–æ 100 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        # –ú–Ω–æ–∂–∏—Ç–µ–ª—å –¥–ª—è –≥–ª—É–±–∏–Ω—ã
        if depth <= 20:
            depth_multiplier = 1
        elif depth <= 50:
            depth_multiplier = 1.5
        elif depth <= 100:
            depth_multiplier = 2
        else:
            depth_multiplier = 3  # –¥–ª—è depth > 100
        
        estimated_cost = len(keyword_ids) * base_cost * depth_multiplier
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞
        try:
            dataforseo_client = get_dataforseo_client()
            status = dataforseo_client.get_status()
            
            current_balance = 0
            if status.get('tasks') and status['tasks'][0].get('result'):
                current_balance = status['tasks'][0]['result'][0].get('money', {}).get('balance', 0)
            
            can_proceed = current_balance >= estimated_cost
            
            return jsonify({
                'success': True,
                'estimated_cost': round(estimated_cost, 4),
                'current_balance': round(current_balance, 2),
                'can_proceed': can_proceed,
                'keywords_count': len(keyword_ids),
                'depth': depth,
                'message': f"–ê–Ω–∞–ª–∏–∑ {len(keyword_ids)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –±—É–¥–µ—Ç —Å—Ç–æ–∏—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ ${estimated_cost:.4f}"
            })
            
        except Exception as e:
            # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–ª–∞–Ω—Å, –≤—Å–µ —Ä–∞–≤–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å
            return jsonify({
                'success': True,
                'estimated_cost': round(estimated_cost, 4),
                'current_balance': None,
                'can_proceed': True,
                'keywords_count': len(keyword_ids),
                'depth': depth,
                'message': f"–ê–Ω–∞–ª–∏–∑ {len(keyword_ids)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –±—É–¥–µ—Ç —Å—Ç–æ–∏—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ ${estimated_cost:.4f}",
                'warning': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–ª–∞–Ω—Å'
            })
            
    except Exception as e:
        log_print(f"‚ùå Error checking SERP cost: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@dataforseo_bp.route('/check-balance', methods=['GET'])
def check_balance():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∏ —Å—Ç–∞—Ç—É—Å–∞ –∞–∫–∫–∞—É–Ω—Ç–∞ DataForSeo"""
    try:
        dataforseo_client = get_dataforseo_client()
        status = dataforseo_client.get_status()
        
        if status.get('tasks') and status['tasks'][0].get('result'):
            result = status['tasks'][0]['result'][0]
            return jsonify({
                'success': True,
                'balance': {
                    'money': result.get('money', {}).get('balance', 0),
                    'currency': 'USD'
                },
                'rates': {
                    'keywords_for_keywords': result.get('rates', {}).get('keywords_data', {}).get('google_ads', {}).get('keywords_for_keywords', {}).get('live', 0.05),
                    'serp': result.get('rates', {}).get('serp', {}).get('live', {}).get('regular', 0.01)
                }
            })
    except ValueError as e:
        return jsonify({
            'success': False,
            'error': f'DataForSeo API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {str(e)}'
        }), 400
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500
        
@dataforseo_bp.route('/debug-serp/<int:log_id>', methods=['GET'])
def debug_serp_log(log_id):
    """DEBUG: –ü–æ–ª–Ω—ã–π –¥–∞–º–ø raw_response –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
    connection = None
    try:
        connection = get_db_connection()
        cursor = connection.cursor()
        
        cursor.execute("SELECT raw_response FROM serp_logs WHERE id = %s", (log_id,))
        log = cursor.fetchone()
        
        if not log:
            return jsonify({'success': False, 'error': 'Log not found'}), 404
        
        raw_response = log.get('raw_response')
        
        if not raw_response:
            return jsonify({'success': False, 'error': 'No raw_response'}), 404
        
        # –ü–∞—Ä—Å–∏–º –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        if isinstance(raw_response, str):
            raw_response = json.loads(raw_response)
        
        items = raw_response['tasks'][0]['result'][0]['items']
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º
        type_stats = {}
        for item in items:
            item_type = item.get('type', 'unknown')
            if item_type not in type_stats:
                type_stats[item_type] = {
                    'count': 0,
                    'examples': []
                }
            type_stats[item_type]['count'] += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ 2 –ø—Ä–∏–º–µ—Ä–∞ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
            if len(type_stats[item_type]['examples']) < 2:
                type_stats[item_type]['examples'].append({
                    'rank_absolute': item.get('rank_absolute'),
                    'domain': item.get('domain'),
                    'title': item.get('title', '')[:80]
                })
        
        cursor.close()
        
        return jsonify({
            'success': True,
            'log_id': log_id,
            'total_items': len(items),
            'type_statistics': type_stats,
            'full_items': items[:5]  # –ü–µ—Ä–≤—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ–ª–Ω–æ—Å—Ç—å—é
        })
        
    except Exception as e:
        log_print(f"‚ùå Debug error: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if connection:
            connection.close()

@dataforseo_bp.route('/locations', methods=['GET'])
def get_locations():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ª–æ–∫–∞—Ü–∏–π"""
    try:
        country = request.args.get('country')
        dataforseo_client = get_dataforseo_client()
        locations = dataforseo_client.get_locations(country)
        
        popular = [
            {'code': 1012852, 'name': 'Kyiv, Kyiv city, Ukraine', 'name_ru': '–ö–∏–µ–≤, –£–∫—Ä–∞–∏–Ω–∞', 'se_domain': 'google.com.ua'},
        ]
        
        return jsonify({
            'success': True,
            'popular': popular,
            'all': locations.get('tasks', [{}])[0].get('result', []) if locations else []
        })
    except ValueError as e:
        return jsonify({'success': False, 'error': f'DataForSeo API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {str(e)}'}), 400
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500
        
@dataforseo_bp.route('/test-serp', methods=['POST'])
def test_serp_single():
    """–¢–µ—Å—Ç–æ–≤—ã–π SERP –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
    try:
        data = request.json
        test_keyword = data.get('keyword', '—É—Ä–æ–∫–∏ –º—É–∑—ã–∫–∏ –∫–∏–µ–≤')
        
        log_print(f"üß™ –¢–ï–°–¢ SERP –¥–ª—è: {test_keyword}")
        
        # –ü–æ–ª—É—á–∞–µ–º DataForSeo –∫–ª–∏–µ–Ω—Ç
        try:
            dataforseo_client = get_dataforseo_client()
        except ValueError as e:
            return jsonify({
                'success': False,
                'error': f'DataForSeo API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {str(e)}'
            }), 400
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∞
        serp_response = dataforseo_client.get_serp(
            keyword=test_keyword,
            location_code=2804,  # Ukraine
            language_code='ru',
            device='desktop',
            os='windows',
            depth=10,  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-10 –¥–ª—è —Ç–µ—Å—Ç–∞
            se_domain='google.com.ua'
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–≤–µ—Ç
        if not serp_response.get('tasks'):
            return jsonify({
                'success': False,
                'error': 'No response from DataForSeo'
            })
        
        task = serp_response['tasks'][0]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
        if task.get('status_code') != 20000:
            return jsonify({
                'success': False,
                'error': task.get('status_message', 'Unknown error'),
                'status_code': task.get('status_code')
            })
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if not task.get('result'):
            return jsonify({
                'success': False,
                'error': 'No results in response'
            })
        
        result = task['result'][0]
        items = result.get('items', [])
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –æ—Ç—á–µ—Ç
        report = {
            'keyword': test_keyword,
            'items_count': len(items),
            'cost': task.get('cost', 0),
            'types_found': [],
            'organic_domains': [],
            'has_ads': False,
            'has_maps': False
        }
        
        for item in items:
            item_type = item.get('type', '')
            
            if item_type not in report['types_found']:
                report['types_found'].append(item_type)
            
            if item_type == 'organic':
                domain = item.get('domain', '')
                if domain:
                    report['organic_domains'].append({
                        'position': item.get('rank_absolute', 0),
                        'domain': domain,
                        'title': item.get('title', '')[:50]
                    })
            elif item_type in ['paid', 'google_ads']:
                report['has_ads'] = True
            elif item_type in ['local_pack', 'maps']:
                report['has_maps'] = True
        
        log_print("‚úÖ –¢–ï–°–¢ SERP —É—Å–ø–µ—à–µ–Ω!")
        log_print(f"   –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {report['items_count']}")
        log_print(f"   –¢–∏–ø—ã: {', '.join(report['types_found'])}")
        log_print(f"   –°—Ç–æ–∏–º–æ—Å—Ç—å: ${report['cost']}")
        
        return jsonify({
            'success': True,
            'message': 'SERP API —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!',
            'report': report,
            'raw_response_preview': {
                'status_code': task.get('status_code'),
                'status_message': task.get('status_message'),
                'cost': task.get('cost'),
                'result_count': result.get('items_count', 0),
                'se_results_count': result.get('se_results_count', 0)
            }
        })
        
    except Exception as e:
        log_print(f"‚ùå Test SERP error: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@dataforseo_bp.route('/languages', methods=['GET'])
def get_languages():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤"""
    try:
        dataforseo_client = get_dataforseo_client()
        languages = dataforseo_client.get_languages()
        
        main_languages = [
            {'code': 'uk', 'name': '–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞'},
            {'code': 'ru', 'name': '–†—É—Å—Å–∫–∏–π'},
            {'code': 'en', 'name': 'English'},
            {'code': 'de', 'name': 'Deutsch'},
            {'code': 'fr', 'name': 'Fran√ßais'},
            {'code': 'es', 'name': 'Espa√±ol'},
            {'code': 'it', 'name': 'Italiano'},
            {'code': 'pl', 'name': 'Polski'},
        ]
        
        # –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤
        all_languages = [
            {'code': 'af', 'name': 'Afrikaans'},
            {'code': 'ak', 'name': 'Akan'},
            {'code': 'sq', 'name': 'Albanian'},
            {'code': 'am', 'name': 'Amharic'},
            {'code': 'ar', 'name': 'Arabic'},
            {'code': 'hy', 'name': 'Armenian'},
            {'code': 'az', 'name': 'Azeri'},
            {'code': 'ban', 'name': 'Balinese'},
            {'code': 'eu', 'name': 'Basque'},
            {'code': 'be', 'name': 'Belarusian'},
            {'code': 'bn', 'name': 'Bengali'},
            {'code': 'bs', 'name': 'Bosnian'},
            {'code': 'bg', 'name': 'Bulgarian'},
            {'code': 'my', 'name': 'Burmese'},
            {'code': 'ca', 'name': 'Catalan'},
            {'code': 'ceb', 'name': 'Cebuano'},
            {'code': 'ny', 'name': 'Chichewa'},
            {'code': 'zh-CN', 'name': 'Chinese (Simplified)'},
            {'code': 'zh-TW', 'name': 'Chinese (Traditional)'},
            {'code': 'hr', 'name': 'Croatian'},
            {'code': 'cs', 'name': 'Czech'},
            {'code': 'da', 'name': 'Danish'},
            {'code': 'nl', 'name': 'Dutch'},
            {'code': 'en', 'name': 'English'},
            {'code': 'es-419', 'name': 'Espa√±ol (Latinoam√©rica)'},
            {'code': 'et', 'name': 'Estonian'},
            {'code': 'ee', 'name': 'Ewe'},
            {'code': 'fo', 'name': 'Faroese'},
            {'code': 'fa', 'name': 'Farsi'},
            {'code': 'fil', 'name': 'Filipino'},
            {'code': 'fi', 'name': 'Finnish'},
            {'code': 'fr', 'name': 'Fran√ßais'},
            {'code': 'fy', 'name': 'Frisian'},
            {'code': 'gaa', 'name': 'Ga'},
            {'code': 'gl', 'name': 'Galician'},
            {'code': 'lg', 'name': 'Ganda'},
            {'code': 'ka', 'name': 'Georgian'},
            {'code': 'de', 'name': 'Deutsch'},
            {'code': 'el', 'name': 'Greek'},
            {'code': 'gu', 'name': 'Gujarati'},
            {'code': 'ht', 'name': 'Haitian'},
            {'code': 'ha', 'name': 'Hausa'},
            {'code': 'he', 'name': 'Hebrew'},
            {'code': 'iw', 'name': 'Hebrew (old)'},
            {'code': 'hi', 'name': 'Hindi'},
            {'code': 'hu', 'name': 'Hungarian'},
            {'code': 'is', 'name': 'Icelandic'},
            {'code': 'bem', 'name': 'IciBemba'},
            {'code': 'ig', 'name': 'Igbo'},
            {'code': 'id', 'name': 'Indonesian'},
            {'code': 'ga', 'name': 'Irish'},
            {'code': 'it', 'name': 'Italiano'},
            {'code': 'ja', 'name': 'Japanese'},
            {'code': 'kn', 'name': 'Kannada'},
            {'code': 'kk', 'name': 'Kazakh'},
            {'code': 'km', 'name': 'Khmer'},
            {'code': 'rw', 'name': 'Kinyarwanda'},
            {'code': 'rn', 'name': 'Kirundi'},
            {'code': 'kg', 'name': 'Kongo'},
            {'code': 'ko', 'name': 'Korean'},
            {'code': 'mfe', 'name': 'Kreol morisien'},
            {'code': 'crs', 'name': 'Kreol Seselwa'},
            {'code': 'kri', 'name': 'Krio'},
            {'code': 'ckb', 'name': 'Kurdish'},
            {'code': 'ky', 'name': 'Kyrgyz'},
            {'code': 'lo', 'name': 'Lao'},
            {'code': 'lv', 'name': 'Latvian'},
            {'code': 'ln', 'name': 'Lingala'},
            {'code': 'lt', 'name': 'Lithuanian'},
            {'code': 'ach', 'name': 'Luo'},
            {'code': 'mk', 'name': 'Macedonian'},
            {'code': 'mg', 'name': 'Malagasy'},
            {'code': 'ms', 'name': 'Malay'},
            {'code': 'ml', 'name': 'Malayam'},
            {'code': 'mt', 'name': 'Maltese'},
            {'code': 'mi', 'name': 'Maori'},
            {'code': 'mr', 'name': 'Marathi'},
            {'code': 'mn', 'name': 'Mongolian'},
            {'code': 'sr-ME', 'name': 'Montenegro'},
            {'code': 'ne', 'name': 'Nepali'},
            {'code': 'nso', 'name': 'Northern Sotho'},
            {'code': 'no', 'name': 'Norwegian'},
            {'code': 'nyn', 'name': 'Nyankole'},
            {'code': 'om', 'name': 'Oromo'},
            {'code': 'ps', 'name': 'Pashto'},
            {'code': 'pcm', 'name': 'Pidgin'},
            {'code': 'pl', 'name': 'Polski'},
            {'code': 'pt', 'name': 'Portugu√™s'},
            {'code': 'pt-BR', 'name': 'Portugu√™s (Brasil)'},
            {'code': 'pt-PT', 'name': 'Portugu√™s (Portugal)'},
            {'code': 'pa', 'name': 'Punjabi'},
            {'code': 'qu', 'name': 'Quechua'},
            {'code': 'ro', 'name': 'Romanian'},
            {'code': 'rm', 'name': 'Romansh'},
            {'code': 'ru', 'name': '–†—É—Å—Å–∫–∏–π'},
            {'code': 'sr', 'name': 'Serbian'},
            {'code': 'sr-Latn', 'name': 'Serbian (Latin)'},
            {'code': 'st', 'name': 'Sesotho'},
            {'code': 'sn', 'name': 'Shona'},
            {'code': 'loz', 'name': 'Silozi'},
            {'code': 'sd', 'name': 'Sindhi'},
            {'code': 'si', 'name': 'Sinhalese'},
            {'code': 'sk', 'name': 'Slovak'},
            {'code': 'sl', 'name': 'Slovenian'},
            {'code': 'so', 'name': 'Somali'},
            {'code': 'es', 'name': 'Espa√±ol'},
            {'code': 'sw', 'name': 'Swahili'},
            {'code': 'sv', 'name': 'Swedish'},
            {'code': 'tg', 'name': 'Tajik'},
            {'code': 'ta', 'name': 'Tamil'},
            {'code': 'te', 'name': 'Telugu'},
            {'code': 'th', 'name': 'Thai'},
            {'code': 'ti', 'name': 'Tigrinya'},
            {'code': 'to', 'name': 'Tonga (Tonga Islands)'},
            {'code': 'lua', 'name': 'Tshiluba'},
            {'code': 'tn', 'name': 'Tswana'},
            {'code': 'tum', 'name': 'Tumbuka'},
            {'code': 'tr', 'name': 'Turkish'},
            {'code': 'tk', 'name': 'Turkmen'},
            {'code': 'uk', 'name': '–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞'},
            {'code': 'ur', 'name': 'Urdu'},
            {'code': 'uz', 'name': 'Uzbek'},
            {'code': 'vi', 'name': 'Vietnamese'},
            {'code': 'wo', 'name': 'Wolof'},
            {'code': 'xh', 'name': 'Xhosa'},
            {'code': 'yo', 'name': 'Yoruba'},
            {'code': 'zu', 'name': 'Zulu'},
        ]
        
        return jsonify({
            'success': True,
            'main': main_languages,
            'all': all_languages
        })

    except ValueError as e:
        return jsonify({'success': False, 'error': f'DataForSeo API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {str(e)}'}), 400
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

def parse_serp_response(serp_response: Dict, campaign_id: int, connection, keyword_id: int = None, keyword_text: str = None) -> Dict:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ SERP –æ—Ç–≤–µ—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    –ò–°–ü–†–ê–í–õ–ï–ù–û: 
    - –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    - –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –í–°–ï–• —Ç–∏–ø–æ–≤ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ DataForSEO
    """
    try:
        if not serp_response.get('tasks'):
            log_print("‚ùå No tasks in SERP response")
            return None
        
        task = serp_response['tasks'][0]
        if task.get('status_code') != 20000:
            log_print(f"‚ùå SERP API error: {task.get('status_message')}")
            return None
        
        if not task.get('result') or len(task['result']) == 0:
            log_print("‚ùå No result in SERP response")
            return None
        
        result = task['result'][0]
        items = result.get('items', [])
        
        if not items:
            log_print("‚ö†Ô∏è No items in SERP response")
            return None
        
        log_print(f"\nüîç –ê–ù–ê–õ–ò–ó SERP –î–õ–Ø: {keyword_text}")
        log_print(f"üìä –í—Å–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(items)}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        has_ads = False
        has_google_maps = False
        has_our_site = False
        has_school_sites = False
        
        # –ü–æ–∑–∏—Ü–∏–∏ –Ω–∞—à–µ–≥–æ —Å–∞–π—Ç–∞
        our_organic_position = None
        our_actual_position = None
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        organic_results = []
        paid_results = []
        maps_results = []
        all_items_parsed = []
        
        # –°—á–µ—Ç—á–∏–∫–∏
        total_organic_sites = 0
        school_sites_count = 0
        organic_position_counter = 0
        
        # –°—á–µ—Ç—á–∏–∫ —Ç–∏–ø–æ–≤ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        from collections import Counter
        type_counter = Counter()
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω –Ω–∞—à–µ–≥–æ —Å–∞–π—Ç–∞
        our_domain = get_campaign_domain(campaign_id, connection)
        school_domains = get_school_domains(connection)
        
        log_print(f"üìå –ù–∞—à –¥–æ–º–µ–Ω: {our_domain or '–ù–ï –£–ö–ê–ó–ê–ù'}")
        log_print(f"\n{'=' * 70}")
        log_print("üìã –î–ï–¢–ê–õ–¨–ù–´–ô –†–ê–ó–ë–û–† –≠–õ–ï–ú–ï–ù–¢–û–í:")
        log_print(f"{'=' * 70}\n")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç
        for idx, item in enumerate(items):
            item_type = item.get('type', 'unknown')
            rank_absolute = item.get('rank_absolute', idx + 1)
            rank_group = item.get('rank_group', idx + 1)
            
            # –°—á–∏—Ç–∞–µ–º —Ç–∏–ø—ã
            type_counter[item_type] += 1
            
            # –û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            domain = (item.get('domain', '') or '').lower()
            url = (item.get('url', '') or '').lower()
            title = item.get('title') or ''
            
            log_print(f"#{idx+1:2d} | Type: {item_type:20s} | rank_abs: {rank_absolute:3d} | rank_group: {rank_group:3d}")
            
            # –†–ï–ö–õ–ê–ú–ù–´–ï –ë–õ–û–ö–ò
            # –°–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: paid, shopping, google_flights, google_hotels
            if item_type in ['paid', 'shopping', 'google_flights', 'google_hotels', 'ads', 'ad']:
                has_ads = True
                clean_domain = domain.replace('www.', '').strip()
                
                paid_results.append({
                    'actual_position': rank_absolute,
                    'domain': clean_domain,
                    'title': title[:100] if title else '',
                    'url': url,
                    'ad_type': item_type  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∏–ø –¥–ª—è debug
                })
                log_print(f"     üí∞ [–†–ï–ö–õ–ê–ú–ê type={item_type}] Domain: {clean_domain}")
                log_print(f"     Title: {title[:60]}")
            
            # GOOGLE MAPS / LOCAL PACK
            # –°–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: local_pack
            elif item_type in ['local_pack', 'map', 'maps', 'google_maps']:
                has_google_maps = True
                maps_results.append({
                    'rank_absolute': rank_absolute,
                    'type': item_type,
                    'title': title[:100] if title else ''
                })
                log_print(f"     üó∫Ô∏è [–ö–ê–†–¢–´] Type: {item_type}")
                
                # Local pack –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Å—Ç
                if item.get('items'):
                    log_print(f"     üìç –ú–µ—Å—Ç –≤ –±–ª–æ–∫–µ: {len(item.get('items', []))}")
                    for place_idx, place in enumerate(item.get('items', [])[:3], 1):
                        place_title = place.get('title', 'No title')
                        log_print(f"        {place_idx}. {place_title}")
            
            # –û–†–ì–ê–ù–ò–ß–ï–°–ö–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´
            elif item_type == 'organic':
                organic_position_counter += 1
                total_organic_sites += 1
                
                clean_domain = domain.replace('www.', '').strip()
                description = item.get('description') or ''
                
                # –ö–†–ò–¢–ò–ß–ù–û: –°–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
                organic_item = {
                    'organic_position': organic_position_counter,
                    'actual_position': rank_absolute,
                    'domain': clean_domain,
                    'title': title[:100] if title else '',
                    'url': url,
                    'description': description[:200] if description else ''
                }
                
                organic_results.append(organic_item)
                
                log_print(f"     üåê [–û–†–ì–ê–ù–ò–ö–ê #{organic_position_counter}] Domain: {clean_domain}")
                log_print(f"     Title: {title[:60]}")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—à–µ–≥–æ —Å–∞–π—Ç–∞
                if our_domain:
                    clean_our_domain = our_domain.replace('www.', '').strip().lower()
                    is_our_site = False
                    
                    # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
                    if clean_our_domain == clean_domain:
                        is_our_site = True
                        log_print(f"        ‚úÖ –¢–û–ß–ù–û–ï –°–û–í–ü–ê–î–ï–ù–ò–ï –î–û–ú–ï–ù–ê")
                    elif clean_our_domain in url:
                        is_our_site = True
                        log_print(f"        ‚úÖ –î–û–ú–ï–ù –ù–ê–ô–î–ï–ù –í URL")
                    elif clean_domain.endswith('.' + clean_our_domain) or clean_our_domain.endswith('.' + clean_domain):
                        is_our_site = True
                        log_print(f"        ‚úÖ SUBDOMAIN MATCH")
                    
                    if is_our_site:
                        has_our_site = True
                        if our_organic_position is None:
                            our_organic_position = organic_position_counter
                            our_actual_position = rank_absolute
                            log_print(f"        üéØ –≠–¢–û –ù–ê–® –°–ê–ô–¢!")
                            log_print(f"        üìç –û—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∞—è –ø–æ–∑–∏—Ü–∏—è: {our_organic_position}")
                            log_print(f"        üìç –§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–∑–∏—Ü–∏—è: {our_actual_position}")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∞–π—Ç–æ–≤ —à–∫–æ–ª
                if clean_domain in school_domains:
                    school_sites_count += 1
                    has_school_sites = True
                    log_print(f"        üè´ –°–ê–ô–¢ –®–ö–û–õ–´-–ö–û–ù–ö–£–†–ï–ù–¢–ê")
            
            # –í–°–ï –û–°–¢–ê–õ–¨–ù–´–ï –¢–ò–ü–´ (–¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
            else:
                log_print(f"     ‚ÑπÔ∏è [{item_type.upper()}]")
                if title:
                    log_print(f"     Title: {title[:60]}")
            
            log_print()  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç
        intent_type = '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π' if has_ads else '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π'
        school_percentage = (school_sites_count / total_organic_sites * 100) if total_organic_sites > 0 else 0
        
        # –ò–¢–û–ì–ò
        log_print(f"\n{'=' * 70}")
        log_print("üìä –ò–¢–û–ì–ò –ê–ù–ê–õ–ò–ó–ê:")
        log_print(f"{'=' * 70}")
        log_print(f"   –í—Å–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(items)}")
        log_print(f"   –¢–∏–ø—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {dict(type_counter)}")
        log_print(f"   –û—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {total_organic_sites}")
        log_print(f"   –†–µ–∫–ª–∞–º–Ω—ã—Ö –±–ª–æ–∫–æ–≤: {len(paid_results)}")
        log_print(f"   –ö–∞—Ä—Ç—ã Google: {'–î–ê' if has_google_maps else '–ù–ï–¢'} ({len(maps_results)} —à—Ç.)")
        log_print(f"   –ù–∞—à —Å–∞–π—Ç: {'–î–ê' if has_our_site else '–ù–ï–¢'}")
        if has_our_site:
            log_print(f"   üìç –û—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∞—è –ø–æ–∑–∏—Ü–∏—è (—Å—Ä–µ–¥–∏ –æ—Ä–≥–∞–Ω–∏–∫–∏): {our_organic_position}")
            log_print(f"   üìç –§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–∑–∏—Ü–∏—è (–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ): {our_actual_position}")
        else:
            log_print(f"   ‚ö†Ô∏è –î–æ–º–µ–Ω '{our_domain}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–æ–π –≤—ã–¥–∞—á–µ")
        log_print(f"   –°–∞–π—Ç—ã —à–∫–æ–ª: {'–î–ê' if has_school_sites else '–ù–ï–¢'} ({school_percentage:.1f}%)")
        log_print(f"   –ò–Ω—Ç–µ–Ω—Ç: {intent_type}")
        log_print(f"{'=' * 70}\n")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º JSON
        parsed_items_json = json.dumps({
            'organic': organic_results,
            'paid': paid_results,
            'maps': maps_results  # ‚Üê –í–û–¢ –û–ù–û –ï–°–¢–¨!
        }, ensure_ascii=False)
        
        analysis_result_json = json.dumps({
            'has_ads': has_ads,
            'has_google_maps': has_google_maps,
            'has_our_site': has_our_site,
            'our_organic_position': our_organic_position,
            'our_actual_position': our_actual_position,
            'has_school_sites': has_school_sites,
            'school_percentage': round(school_percentage, 1),
            'intent_type': intent_type,
            'total_organic': total_organic_sites,
            'paid_count': len(paid_results),
            'maps_count': len(maps_results)
        }, ensure_ascii=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        if connection and keyword_id:
            try:
                cursor = connection.cursor()
                
                request_data = task.get('data', {})
                if isinstance(request_data, list) and len(request_data) > 0:
                    request_params = request_data[0]
                else:
                    request_params = {}
                
                insert_query = """
                    INSERT INTO serp_logs (
                        keyword_id, keyword_text, location_code, language_code,
                        device, depth, total_items, organic_count, paid_count,
                        maps_count, shopping_count, has_ads, has_maps,
                        has_our_site, has_school_sites, intent_type,
                        school_percentage, cost, raw_response, parsed_items,
                        analysis_result
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                        %s, %s, %s, %s, %s, %s, %s, %s
                    )
                """
                
                insert_values = (
                    keyword_id,
                    keyword_text or '',
                    request_params.get('location_code', 0),
                    request_params.get('language_code', ''),
                    request_params.get('device', 'desktop'),
                    request_params.get('depth', 0),
                    len(items),
                    total_organic_sites,
                    len(paid_results),
                    len(maps_results),
                    0,
                    has_ads,
                    has_google_maps,
                    has_our_site,
                    has_school_sites,
                    intent_type,
                    school_percentage,
                    task.get('cost', 0),
                    json.dumps(serp_response, ensure_ascii=False),
                    parsed_items_json,
                    analysis_result_json
                )
                
                cursor.execute(insert_query, insert_values)
                inserted_id = cursor.lastrowid
                connection.commit()
                cursor.close()
                
                log_print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î (serp_logs), ID: {inserted_id}\n")
                
            except Exception as e:
                log_print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {str(e)}")
                import traceback
                traceback.print_exc()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return {
            'has_ads': has_ads,
            'has_school_sites': has_school_sites,
            'has_google_maps': has_google_maps,
            'has_our_site': has_our_site,
            'our_organic_position': our_organic_position,
            'our_actual_position': our_actual_position,
            'intent_type': intent_type,
            'stats': {
                'total_organic': total_organic_sites,
                'paid_count': len(paid_results),
                'maps_count': len(maps_results),
                'school_percentage': round(school_percentage, 1)
            }
        }
        
    except Exception as e:
        log_print(f"‚ùå Error parsing SERP: {str(e)}")
        import traceback
        traceback.print_exc()
        return None
        
def get_campaign_domain(campaign_id: int, connection) -> str:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –¥–æ–º–µ–Ω —Å–∞–π—Ç–∞ –¥–ª—è –∫–∞–º–ø–∞–Ω–∏–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã campaign_sites
    """
    try:
        cursor = connection.cursor()
        cursor.execute("""
            SELECT domain 
            FROM campaign_sites 
            WHERE campaign_id = %s
        """, (campaign_id,))
        
        result = cursor.fetchone()
        cursor.close()
        
        if result and result['domain']:
            domain = result['domain'].lower()
            # –£–±–∏—Ä–∞–µ–º www. –µ—Å–ª–∏ –µ—Å—Ç—å
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        
        return None
        
    except Exception as e:
        log_print(f"‚ùå Error getting campaign domain: {e}")
        return None


def get_school_domains(connection) -> set:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ —à–∫–æ–ª-–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –∏–∑ –ë–î
    """
    try:
        cursor = connection.cursor()
        cursor.execute("""
            SELECT domain 
            FROM school_sites 
            WHERE is_active = TRUE
        """)
        
        results = cursor.fetchall()
        cursor.close()
        
        # –°–æ–∑–¥–∞–µ–º set –¥–æ–º–µ–Ω–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        domains = set()
        for row in results:
            if row['domain']:
                domain = row['domain'].lower()
                # –£–±–∏—Ä–∞–µ–º www. –µ—Å–ª–∏ –µ—Å—Ç—å
                if domain.startswith('www.'):
                    domain = domain[4:]
                domains.add(domain)
        
        return domains
        
    except Exception as e:
        log_print(f"‚ö†Ô∏è Error getting school domains: {e}")
        # –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü—ã –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π set
        return set()